# 随机梯度下降SGD

- [返回顶层目录](../../SUMMARY.md#目录)
- [返回上层目录](numerical-calculation-and-optimization.md)


几乎所有的深度学习算法都用到了随机梯度下降，它是梯度下降算法的拓展。

机器学习中反复出现的一个问题是：好的泛化需要大的训练集，但是，训练集越大，计算代价也越大。

# 大数据中一般梯度下降算法的问题

机器学习算法中，代价函数通常可分解为各样本的代价函数的总和。例如，训练数据的负对数条件似然可写成
$$
J(\theta)=\mathbb{E}_{x,y\sim \hat{p}_{data}}L(x,y,\theta)=\frac{1}{m}\sum^{m}_{i=1}L(x^{(i)},y^{(i)},\theta)
$$
其中，L是每个样本的损失：
$$
L(x,y,\theta)=-log\ p(y|x;\theta)
$$
对于这些相加的代价函数，梯度下降需要计算
$$
\bigtriangledown_{\theta}J(\theta)=\frac{1}{m}\sum^{m}_{i=1}\bigtriangledown_{\theta}L(x^{(i)},y^{(i)},\theta)
$$
这个运算的计算复杂度是O(m)。随着训练集规模增长为数十亿的样本，计算一步梯度也会消耗相当长的时间。

# 随机梯度下降的原理

随机梯度下降的核心是，**梯度是期望。期望可使用小规模的样本近似估计**。

具体步骤：我们在每一步都从训练集中均匀抽出一**小批量（minibatch）**样本
$$
\mathbb{B}=\{x^{(1)},...,x^{(m^{'})}\}
$$
，小批量的数目m'通常是一个相对较小的数，从一到几百。重要的是，当训练集大小m增长时，m'通常是固定的。我们可能在拟合几十亿样本时，每次更新计算只用到几百个样本。

梯度的估计可以表示成
$$
g=\frac{1}{m^{'}}\bigtriangledown_{\theta}\sum^{m^{'}}_{i=1}L(x^{(i)},y^{(i)},\theta)
$$
使用来自小批量$\mathbb{B}$的样本。然后，随机梯度下降算法使用如下的梯度下降算法估计：
$$
\theta=\theta-\epsilon g
$$
其中，$\epsilon$是学习率。

# 随机梯度下降的性质

梯度下降往往被认为很慢或不可靠。

在以前，将梯度下降应用到非凸优化问题被认为很鲁莽或者没有原则。而现在，在深度学习中，使用梯度下降的训练效果很不错。虽然优化算法不一定能保证在合理的时间内达到一个局部最小值，但它通常能及时地找到代价函数一个很小的值，并且是有用的。

在深度学习之外，随机梯度下降有很多重要的应用。它是在大规模数据上训练大型线性模型的主要方法。对于固定大小的模型，每一步随机梯度下降更新的计算量不取决于训练集的大小m。在实践中，当训练集大小增长时，我们通常会使用一个更大的模型，但是这并非是必须的。**达到收敛模型所需的更新次数通常会随着训练集规模增大而增加。然而，当m趋于无穷大时，该模型最终会随机梯度下降抽样完训练集上所有样本之前收敛到可能的最优测试误差。继续增加m不会延长达到模型可能的最优测试误差的时间**。从这点来看，我们可以认为用SGD训练模型的渐进代价是关于m的函数的O(1)级别。



# SGD过程中的噪声如何帮助避免局部极小值和鞍点？

https://zhuanlan.zhihu.com/p/36816689






# 参考资料

* 《深度学习》Goodfellow

本文参考了此书的第五章5.9小节“随机梯度下降”。









