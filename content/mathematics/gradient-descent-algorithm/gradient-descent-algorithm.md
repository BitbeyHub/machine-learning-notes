# 梯度下降算法

- [返回顶层目录](../../SUMMARY.md#目录)
- [各类梯度下降算法的演化](#各类梯度下降算法的演化)
- [随机梯度下降SGD](sgd.md)
- [动量法Momentum](momentum.md)
- [牛顿动量Nesterov](nesterov.md)
- [AdaGrad](adagrad.md)
- [RMSprop](rmsprop.md)
- [Adadelta](adadelta.md)
- [Adam](adam.md)
- [Nadam](nadam.md)
- [AMSGrad](amsgrad.md)
- [AdasMax](adamax.md)

![optimization-on-loss-surface-contours](pic/optimization-on-loss-surface-contours.gif)

# 各类梯度下降算法的演化

![revolution-of-gradient-descent](pic/revolution-of-gradient-descent.jpeg)





# 参考资料

* [Deep Learning 之 最优化方法](https://blog.csdn.net/BVL10101111/article/details/72614711)
* [从 SGD 到 Adam —— 深度学习优化算法概览(一)](https://zhuanlan.zhihu.com/p/32626442)
* [深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）](https://zhuanlan.zhihu.com/p/22252270)