# ANN人工神经网络

* [返回顶层目录](../../SUMMARY.md#目录)



神经网络为什么可以（理论上）拟合任何函数？

<https://www.zhihu.com/question/268384579>



# 激活函数



[理解神经网络的激活函数](https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&mid=2247483977&idx=1&sn=401b211bf72bc70f733d6ac90f7352cc&chksm=fdb69fdecac116c81aad9e5adae42142d67f50258106f501af07dc651d2c1473c52fad8678c3&mpshare=1&scene=1&srcid=0508JKgf4ThI1pfcRQSyK3Q4#rd)



卷积网络的激活函数sigmod和relu有什么区别？

使用sigmod函数会导致将近一半的神经元被激活。不太符合人类脑活动工程学。
而relu函数在这方面神似，自动引入稀疏性，相当于无监督预练习。

