# 决策树

* [返回顶层目录](../../SUMMARY.md#目录)
* [决策树概述](#决策树概述)




[算法杂货铺——分类算法之决策树(Decision tree)](http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html)

[机器学习算法之决策树](https://www.jianshu.com/p/6eecdeee5012)

[决策树与随机森林](http://www.cnblogs.com/fionacai/p/5894142.html)



[理解决策树](https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&mid=2247484827&idx=1&sn=043d7d0159baaddfbf92ed78ee5b1124&chksm=fdb6980ccac1111a9faeae7f517fee46a1dfab19612f76ccfe5417487b3f090ab8fc702d18b8&scene=0#rd)



# 决策树概述



## 摘要

这里讨论一种被广泛使用（并且建议优先使用）的分类算法——[决策树](http://en.wikipedia.org/wiki/Decision_tree)（decision tree）。相比贝叶斯算法，决策树的优势在于构造过程不需要任何领域知识或参数设置，因此在实际应用中，对于探测式的知识发现，决策树更加适用。

当一条记录有若干不同的方式划分为目标类的一部分时，适合用单条线来发现类别之间边界的统计学方法是无力的。而决策树能够成功地达到这一目标。

## 引入并定义决策树

通俗来说，决策树分类的思想类似于找对象。现想象一个女孩的母亲要给这个女孩介绍男朋友，于是有了下面的对话：

> **女儿**：多大年纪了？
>
> 母亲：26。
>
> **女儿**：长的帅不帅？
>
> 母亲：挺帅的。
>
> **女儿**：收入高不？
>
> 母亲：不算很高，中等情况。
>
> **女儿**：是公务员不？
>
> 母亲：是，在税务局上班呢。
>
> **女儿**：那好，我去见见。

这个女孩的决策过程就是典型的分类树决策。相当于通过年龄、长相、收入和是否公务员对将男人分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么这个可以用下图表示女孩的决策逻辑（**声明：此决策树纯属为了写文章而YY的产物，没有任何根据，也不代表任何女孩的择偶倾向，请各位女同胞莫质问我^_^**）：

![girl-select-boy](pic/girl-select-boy.png)

上图完整表达了这个女孩决定是否见一个约会对象的策略，其中绿色节点表示判断条件，橙色节点表示决策结果，箭头表示在一个判断条件在不同情况下的决策路径，图中红色箭头表示了上面例子中女孩的决策过程。

这幅图基本可以算是一颗决策树，说它“基本可以算”是因为图中的判定条件没有量化，如收入高中低等等，还不能算是严格意义上的决策树，如果将所有条件量化，则就变成真正的决策树了。

有了上面直观的认识，我们可以正式定义决策树了：

**决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。**

可以看到，决策树的决策过程非常直观，容易被人理解。目前决策树已经成功运用于医学、制造产业、天文学、分支生物学以及商业等诸多领域。知道了决策树的定义以及其应用方法，下面介绍决策树的构造算法。

## 决策树的构造

不同于贝叶斯算法，决策树的构造过程不依赖领域知识，它使用属性选择度量来选择将元组最好地划分成不同的类的属性。所谓决策树的构造就是进行属性选择度量确定各个特征属性之间的拓扑结构。

构造决策树的关键步骤是分裂属性。所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。

分裂属性分为三种不同的情况：

1. 属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。
2. 属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。
3. 属性是连续值。此时确定一个值作为分裂点split_point，按照>split_point和<=split_point生成两个分支。

构造决策树的关键性内容是进行**属性选择度量**，属性选择度量是一种选择分裂准则，是将给定的类标记的训练集合的数据划分D“最好”地分成个体类的[启发式方法](http://en.wikipedia.org/wiki/Heuristic_(computer_science))，它决定了拓扑结构及分裂点split_point的选择。

属性选择度量算法有很多，一般使用自顶向下递归分治法，并采用不回溯的[贪心策略](http://en.wikipedia.org/wiki/Greedy_algorithm)(决策树的**实质**就是贪心算法，只管当前最好，今朝有酒今朝醉，哪管日后洪水滔天)。这里介绍[ID3](http://en.wikipedia.org/wiki/ID3_algorithm)、[C4.5](http://en.wikipedia.org/wiki/C4.5_algorithm)和CART三种常用算法。

### ID3算法

 从[信息论](http://en.wikipedia.org/wiki/Information_theory)知识中我们直到，期望信息越小，[信息增益](http://en.wikipedia.org/wiki/Information_gain)越大，从而纯度越高。所以ID3算法的核心思想就是以**信息增益**度量属性选择，选择分裂后信息增益最大的属性进行分裂。下面先定义几个要用到的概念。

设D为用类别对训练元组进行的划分，则D的[熵](http://en.wikipedia.org/wiki/Entropy)（entropy）表示为：
$$
info(D)=-\sum_{i=1}^{m}p_i log_2p_i
$$
其中$p_i$表示第i个类别在整个训练元组中出现的概率，可以用属于此类别元素的数量除以训练元组元素总数量作为估计。熵的实际意义表示是D中元组的类标号所需要的平均信息量。

现在我们假设将训练元组D按属性A进行划分，则A对D划分的期望信息为：
$$
info_A(D)=\sum_{j=1}^{v}\frac{|D_j|}{|D|}info(D_j)
$$
而信息增益即为两者的差值：
$$
gain(A)=info(D)-info_A(D)
$$
ID3算法就是在每次需要分裂时，计算每个属性的增益率，然后选择增益率最大的属性进行分裂。下面我们继续用SNS社区中不真实账号检测的例子说明如何使用ID3算法构造决策树。为了简单起见，我们假设训练集合包含10个元素：

![SNS-ID-check](pic/SNS-ID-check.png)

其中s、m和l分别表示小、中和大。

设L、F、H和R表示日志密度、好友密度、是否使用真实头像和账号是否真实，下面计算各属性的信息增益。
$$
\begin{aligned}
info(D)&=-0.7log_2{0.7}-0.3log_2{0.3}\\
&=0.7\times 0.51+0.3\times 1.74\\
&=0.879
\end{aligned}
$$

$$
\begin{aligned}
info_L(D)=&0.3\times (-\frac{0}{3}log_2{\frac{0}{3}}-\frac{3}{3}log_2\frac{3}{3})\\
+&0.4\times (-\frac{1}{4}log_2\frac{1}{4}-\frac{3}{4}log_2\frac{3}{4})\\
+&0.3\times (-\frac{1}{3}log_2\frac{1}{3}-\frac{2}{3}log_2\frac{2}{3})\\
=&0+0.326+0.277\\
=&0.603
\end{aligned}
$$

$$
gain(L)=0.879-0.603=0.276
$$

因此日志密度的信息增益是0.276。

用同样方法得到F和H的信息增益分别为0.033和0.553。

因为F具有最大的信息增益，所以第一次分裂选择F为分裂属性，分裂后的结果如下图表示：

![SNS-ID-check-2](pic/SNS-ID-check-2.png)

在上图的基础上，再递归使用这个方法计算子节点的分裂属性，最终就可以得到整个决策树。

上面为了简便，将特征属性离散化了，其实日志密度和好友密度都是连续的属性。对于特征属性为**连续值**，可以如此使用ID3算法：

先将D中元素按照特征属性排序，则每两个相邻元素的中间点可以看做潜在分裂点，从第一个潜在分裂点开始，分裂D并计算两个集合的期望信息，具有最小期望信息的点称为这个属性的最佳分裂点，其信息期望作为此属性的信息期望。

ID3算法存在一个问题，就是偏向于多值属性，例如，如果存在唯一标识属性ID，则ID3会选择它作为分裂属性，这样虽然使得划分充分纯净，但这种划分对分类几乎毫无用处。

### C4.5算法

ID3算法存在一个问题，就是偏向于多值属性，例如，如果存在唯一标识属性ID，则ID3会选择它作为分裂属性，这样虽然使得划分充分纯净，但这种划分对分类几乎毫无用处。ID3的后继算法C4.5使用[**信息增益率**](http://en.wikipedia.org/wiki/Information_gain_ratio)（gain ratio）的信息增益扩充，试图克服这个偏倚。

C4.5算法首先定义了“分裂信息”，其定义可以表示成：
$$
split\_info_A{D}=-\sum_{j=1}^{v}\frac{D_j}{D}log_2\frac{D_j}{D}
$$
其中各符号意义与ID3算法相同，然后，增益率被定义为：
$$
gain\_ratio(A)=\frac{gain(A)}{split_info(A)}
$$
C4.5选择具有最大增益率的属性作为分裂属性，其具体应用与ID3类似，不再赘述。

### CART算法

用**基尼系数**(Gini Index)作为纯度度量

**GINI系数**

1. 是一种不等性度量；
2. 通常用来度量收入不平衡，可以用来度量任何不均匀分布；
3. 是介于0~1之间的数，0-完全相等，1-完全不相等；
4. **总体内包含的类别越杂乱，GINI指数就越大（跟熵的概念很相似） **

**基尼不纯度指标**

在CART算法中，基尼不纯度表示一个随机选中的样本在子集中被分错的可能性。基尼不纯度为这个样本被选中的概率乘以它被分错的概率。当一个节点中所有样本都是一个类时，基尼不纯度为零。
假设y的可能取值为1, 2, ..., m，令pi是样本被赋予i的概率，则**基尼指数**可以通过如下计算：
$$
Gini(p)=\sum_{i=1}^{m}p_i(1-p_i)=1-\sum_{i=1}^{m}p_i^2
$$
基尼不纯度的大概意思是：一个随机事件变成它的对立事件的概率

例如一个随机事件X ，P(X=0) = 0.5, P(X=1)=0.5，那么基尼不纯度就为P(X=0)×(1-P(X=0))+P(X=1)×(1-P(X=1))=0.5。

一个随机事件Y ，P(Y=0)=0.1, P(Y=1)=0.9，则基尼不纯度就为P(Y=0)×(1 - P(Y=0))+P(Y=1)×(1-P(Y=1))=0.18。

很明显X比Y更混乱，因为两个都为0.5，很难判断哪个发生。而Y就确定得多，Y=0发生的概率很大。而基尼不纯度也就越小。

所以，和熵可以作为衡量系统混乱程度的标准一样，基尼不纯度也可以作为衡量系统混乱程度的标准。

**cart分类树中的基尼指数**

如果训练数据集D根据特征A是否取某一可能值a被分割为D1和D2两部分，则在特征A的条件下，集合D的基尼指数定义为
$$
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
$$
基尼指数$Gini(D)$表示集合D的不确定性，基尼指数$Gini(D,A)$表示经过A=a分割后集合D的不确定性。基尼指数越大，样本的不确定性也就越大。

**熵VS基尼指数**

随机变量的熵表达形式：
$$
H(X)=-\sum_{n=1}^{N}p_ilog(p_i)
$$
随机变量的基尼系数表达形式：
$$
Gini(p)=\sum_{i=1}^{m}p_i(1-p_i)=1-\sum_{i=1}^{m}p_k^2
$$
主要区别在于，熵达到峰值的过程要相对慢一些。因此，熵对于混乱集合的判罚要更重一些。

![difference-between-gini-index-and-entropy](pic/difference-between-gini-index-and-entropy.jpg)

同样可以根据计算基尼不纯度公式计算。实际使用中，熵值对于混乱的惩罚更小，使用熵的情况更多。

### CHAID决策树

使用**卡方检验**进行评估

### 不同算法对比



老师，在CART树中，基尼不纯度和基尼系数是一个意思吗？



## 剪枝

问题

过度拟合(over fitting)

对策

前剪枝(pre prune)：先设定好规则，一旦数据符合这个规则就被剪枝

后剪枝(post prune): 子树替换，子树提升

​         奥卡姆剃刀(Occam’s Razor)：





**过度拟合：**它可能会变得过于针对训练数据，其熵值与真实情况相比可能会有所降低。**剪枝**的过程就是对具有相同父节点的一组节点进行检查，判断如果将其合并，熵的增加量是否会小于某个指定的阈值。如果确实如此，则这些叶节点会被合并成一个单一的节点，合并后的新节点包含了所有可能的结果值。这种做法有助于过度避免过度拟合的情况，使得决策树做出的预测结果，不至于比从数据集中得到的实际结论还要特殊：









# 参考文献

* [算法杂货铺——分类算法之决策树(Decision tree)](http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html)

“决策树概述”很大部分参考了此文章。

* [CART算法中的基尼指数](http://blog.sina.com.cn/s/blog_14a25e82b0102xc6l.html)

"CART算法"一节参考了此文章。

